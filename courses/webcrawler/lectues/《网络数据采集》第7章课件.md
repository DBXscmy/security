
# 网络数据采集

# 第8章 构建分布式爬虫系统

- 讲师姓名：
- 授课时间：
- 共32课时，第28-32课时

## 课前引导

今天我们将介绍第8讲，构建分布式爬虫系统。

大家知道，当今以大数据、云计算、物联网为支撑技术的第三次数字化浪潮已经席卷了全球各个产业。网络上各类信息的产生速度和数据容量不断攀升，传统的爬虫程序在采集速度和存储容量上存在瓶颈，无法满足大数据时代的数据采集需求。

如何解决这一问题呢？

可以这样考虑，既然为了解决海量数据的计算和存储诞生了分布式计算和分布式存储技术，那么能否参考这些技术设计网络爬虫程序呢？

答案是肯定的。

这一讲，我们将介绍使用Redis-scrapy模块，扩展scrapy爬虫框架，构建分布式网络爬虫的方法。



## 上节回顾


## 本节课程主要内容

内容列表：

- 本节目标
- 分布式爬虫系统概述
- Redis-scrapy介绍
- 使用Redis-scrapy构建分布式爬虫系统
- 本节总结
- 课后练习

### 本节目标


### 分布式爬虫系统概述


#### 知识讲解

首先介绍分布式爬虫系统概述。

通用的分布式系统，是指一组计算机，透过网络相互连接传递消息与通信后并协调它们的行为而形成的系统。组件之间彼此进行交互以实现一个共同的目标。这个系统将需要进行大量计算的任务数据分割成小块，由多台计算机分别计算，再上传运算结果后，将结果统一合并得出最终结果。
分布式爬虫系统是用于完成数据采集任务的分布式爬虫系统。根据不同机器之间的协同方式不同，分布式爬虫架构可分为两种：

- 第一种是主从式分布爬虫；
- 第二种是对等式分布爬虫。

在主从式分布爬虫的架构中，不同的服务器承担不同的角色分工，其中有一台主服务器（也称URL服务器）专门负责待爬取URL的分发，其他服务器作为从服务器负责实际网页访问和资源下载。

主服务器不仅维护着待爬取URL队列，还担负着从服务器的负载均衡任务，确保各个从服务器的负荷大致相同。其架构如下图所示：

![主从式分布式架构](images/08/主从式分布式架构.png)

Google在早期即采用此种主从分布式爬虫，在这种架构中，因为URL服务器承担很多管理任务，同时待抓取URL队列数量巨大，所以URL服务器容易成为整个系统的瓶颈。

在第二类，对等式分布爬虫架构中，服务器之间不存在分工差异，每台服务器承担相同的功能，各自负担一部分URL的抓取工作，下图是其中一种对等式分布爬虫，Mercator爬虫采用此种体系结构。

![对等式分布式架构](images/08/对等式分布式架构.png)

由于没有主服务器存在，服务器间的任务分工就成为共同完成的任务，每台服务器都会根据一定的算法（例如基于hash的某种分支规则），自己判断某个URL是否应该由自己来抓取，或者将这个URL传递给相应的服务器。
 


#### 案例与应用

暂无

#### 模块练习与答案

见习题集

#### 内容小结

这一节主要介绍了分布式系统的基本概念和工作原理。

### Redis-scrapy介绍


#### 知识讲解

以上，我们介绍了分布式爬虫的概念和基本架构。在实际构建分布式爬虫时，人们往往会借助高性能的键值数据库、消息队列或非阻塞I/O调度来实现分布式爬虫系统的调度系统，并以此为基础构建分布式爬虫系统。

下面，我们将重点介绍以redis数据库为核心的分布式爬虫模块redis-scrapy，并使用它构建我们自己的分布式爬虫系统。

在介绍Redis-scrapy之前，我们先回顾以下scrapy爬虫框架:

- Scrapy中有一个本地爬取队列Queue，这个队列是利用deque模块实现的。
- 如果有新的URL请求生成就会放到队列里面，随后URL请求被Scheduler调度，交给Downloader执行爬取。
- 当待爬取队列中的URL极多时，为了提高爬虫的性能，我们需要多个的schedular从队列中取出URL，同时也需要有多个downloader去指向URL访问。这时队列的I/O性能必须足够好，才能满足多台服务器中scrapy爬虫读、写URL的需求。


所以人们很自然的就会想到使用高性能的内存数据库Redis作为存储URL的“队列”。

 Redis是一种key-value存储系统。可用于缓存、事件发布或订阅、高速队列等场景。它支持多种数据结构，例如列表（List）、集合（Set）、有序集合（Sorted Set）等，存取的操作非常简单，有利于实现去重和满足一定存取规则的URL队列。另外使用数据库存放URL，还可以防止爬虫意外中断后的继续爬取。
 
Redis-scrapy模块就是基于Redis和scrapy的一个分布式爬虫支持模块。它可以帮助我们轻松实现分布式爬虫系统。

Redis-scrapy的架构如下图所示：

![redis-scrapy架构](images/08/redis-scrapy架构.png)

不难看出，Redis-scrapy是在原有scrapy架构的基础上，增加了Redis组件。它取代Scrapy queue作为待爬取URL的请求队列，这一改变使得多个scrapy spider能够同时获取同一个URL队列中的待爬取请求。

为了从Redis中读取待爬取的请求，Redis-scrapy模块改变了原有scrapy中的schedular和spdier，使其能够直接读取redis数据。同时还增加了redis中的请求去重功能。

要使用redis-scrapy需要安装Redis数据库、Redis-scrapy模块。


下面我们将通过实际操作演示，进行详细介绍。

**准备工作**

1. 安装 Redis


- windows 下安装redis

Redis Windows安装版从 https://github.com/microsoftarchive/redis/releases 下载。

可以选择压缩包或安装包例如：

Redis-x64-3.0.504.msi 或 Redis-x64-3.0.504.zip。


- Ubuntu Linux下的redis安装


安装之前要到 https://redis.io/ 下载Redis安装文件。

在 Ubuntu 系统安装 Redis ，可以使用以下命令:

> sudo apt-get update

> sudo apt-get install redis-server

随着docker容器技术的普及，在服务器的docker容器中使用redis也是一个不错的选择。如果你已经安装了docker程序，并能够运行，可以运行下列命令pull 一个redis docker 镜像：

```
sudo docker pull redis

# pull 完成后，启动镜像用下列命令运行

sudo docker run -p 6379:6379 -d redis:latest redis-server

## 或者使用下列命令

菜鸟教程：
docker run -p 6379:6379 -v $PWD/data:/data  -d redis:3.2 redis-server --appendonly yes

## 命令说明：
## -p 6379:6379 : 将容器的6379端口映射到主机的6379端口
## -v $PWD/data:/data : 将主机中当前目录下的data挂载到容器的/data
## redis-server --appendonly yes : 在容器执行redis-server启动命令，并打开redis持久化配置

## 连接redis的几种方式：

docker ps 
# 查看出容器id，例如为 91718310db7d

sudo docker exec -ti 91718310db7d redis-cli -h localhost -p 6379

# 结果显示 localhost:6379> 

注意，这个是容器运行的ip，可通过 docker inspect redis_s | grep IPAddress 查看

# 如果连接远程：

docker exec -it redis_s redis-cli -h 192.168.1.100 -p 6379 -a your_password //如果有密码 使用 -a参数
# 结果显示 192.168.1.100:6379> 

```

如果需要挂载配置文件和数据文件，需要预先创建文件目录和文件，例如：

- ~/docker/redis/conf/redis.conf，内容可为空
-  ~/docker/redis/data/

然后，用下列命令：

```
sudo docker run -d --privileged=true -h localhost -p 6379:6379 -v ~/docker/redis/conf/redis.conf:/etc/redis/redis.conf -v ~/docker/redis/data/:/data --name redis1  redis:latest redis-server /etc/redis/redis.conf --appendonly yes

参数说明：

--privileged=true：容器内的root拥有真正root权限，否则容器内root只是外部普通用户权限

-v /docker/redis/conf/redis.conf:/etc/redis/redis.conf：映射配置文件

-v /docker/redis/data:/data：映射数据目录

redis-server /etc/redis/redis.conf：指定配置文件启动redis-server进程

--appendonly yes：开启数据持久化。


```

如果要删除无用的docker 容器可用命令：

```
sudo docker rm $(sudo docker ps -aq)
```

2. 启动 Redis

在windows下，启动redis命令为：

- 先进入redis安装目录，默认是program files/redis
- 运行下列命令：

```
redis-server redis.windows.conf


```

3. 运行控制台命令

```
redis-cli.exe -h 127.0.0.1 -p 6379
```

- 尝试存储键值

```
set mykey abc
#
get mykey
#"abc"

# 查看所有键值命令
keys *

# 查看数据库键值数量命令
dbsize

```

在ubuntu linux下，启动redis 命令为：redis-server

之后，运行下列命令会进入redis工作环境下：


```
redis-cli

# 将打开以下终端：

redis 127.0.0.1:6379>

```


以上说明我们已经成功安装了redis。

 
4. 安装Redis Desktop Manager工具

这个工具可以令我们更容易观察redis中的key-value变化。

Windows下安装运行步骤如下：

- Install Microsoft Visual C++ 2017 x64 (If you have not already)
- Download Windows Installer from http://redisdesktop.com/download (Requires subscription)
- Run downloaded installer

5. 安装 scrapy-redis 组件

打开Anaconda3 命令行，建立一个虚拟环境 scrapyredisws 并安装scrapy-redis，命令如下：

```
(base) C:\Users\xxx>conda create -n scrapyredisws scrapy-redis

```

如果出现No module named win32api，需要运行下列命令
```
pip install pypiwin32
```


#### 模块练习与答案

见习题集

#### 内容小结

本节介绍了使用scrapy-redis 搭建分布式爬虫需要的准备工作，主要有以下几步：

1. 安装 Redis
2. 启动 Redis
3. 运行控制台命令
4. 安装Redis Desktop Manager工具
5. 安装 scrapy-redis 组件

### 使用Redis-scrapy构建分布式爬虫系统


#### 知识讲解

上面我们通过实际操作，介绍了redis-scrpy模块的安装和测试。下面我们将介绍如何使用它来构建自己的分布式爬虫系统。对于这部分内容，我们还是通过实际操作来详细介绍。

#### 案例与应用



下面我们将构建项目tutorial，使其能够从redis中读取url，并将结果返回给redis，以键值列表形式进行存储。

1. 启动redis数据库

```
cd c:\program files\redis\
# 启动服务
redis-server redis.windows.conf
# 进入控制台
redis-cli
# 查看所有keys
keys *
# 查看数据大小
dbsize

# 将redis-server 安装为服务，防止关闭窗口后，server关闭。
redis-server --service-install redis.windows-service.conf --loglevel verbose ( 安装redis服务 )

输入：redis-server --service-start ( 启动服务 )

输入：redis-server --service-stop （停止服务）
```
2. 建立tutorial项目，使其从redis数据库中读取url



- 首先运行下列命令，激活 scrapyredisws 这个虚拟目录,并进入这个目录

```
(base) C:\Users\leo>conda activate scrapyredisws

(scrapyredisws) C:\Users\leo>cd Anaconda3\envs\scrapyredisws

(scrapyredisws) C:\Users\leo\Anaconda3\envs\scrapyredisws>
```

- 接着运行下列命令建立项目 tutorial

```
scrapy startproject tutorial

# 生成的目录结构与scrapy 项目相同
```

- 编写 tutorial/tutorial/settings.py 文件，如下:

```
# -*- coding: utf-8 -*-

# Scrapy settings for tutorial project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'tutorial'

SPIDER_MODULES = ['tutorial.spiders']
NEWSPIDER_MODULE = 'tutorial.spiders'


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = 'tutorial (+http://www.yourdomain.com)'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    'tutorial.middlewares.TutorialSpiderMiddleware': 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    'tutorial.middlewares.TutorialDownloaderMiddleware': 543,
#}

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
#ITEM_PIPELINES = {
#    'tutorial.pipelines.TutorialPipeline': 300,
#}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

# Enables scheduling storing requests queue in redis.
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

SCHEDULER_PERSIST = True
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"

# Ensure all spiders share same duplicates filter through redis.
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

# Default requests serializer is pickle, but it can be changed to any module
# with loads and dumps functions. Note that pickle is not compatible between
# python versions.
# Caveat: In python 3.x, the serializer must return strings keys and support
# bytes as values. Because of this reason the json or msgpack module will not
# work by default. In python 2.x there is no such issue and you can use
# 'json' or 'msgpack' as serializers.
#SCHEDULER_SERIALIZER = "scrapy_redis.picklecompat"

# Don't cleanup redis queues, allows to pause/resume crawls.
#SCHEDULER_PERSIST = True

# Schedule requests using a priority queue. (default)
#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'

# Alternative queues.
#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'
#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'

# Max idle time to prevent the spider from being closed when distributed crawling.
# This only works if queue class is SpiderQueue or SpiderStack,
# and may also block the same time when your spider start at the first time (because the queue is empty).
#SCHEDULER_IDLE_BEFORE_CLOSE = 10

# Store scraped item in redis for post-processing.

ITEM_PIPELINES = {
    'tutorial.pipelines.TutorialPipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}
# The item pipeline serializes and stores the items in this redis key.
#REDIS_ITEMS_KEY = '%(spider)s:items'

# The items serializer is by default ScrapyJSONEncoder. You can use any
# importable path to a callable object.
#REDIS_ITEMS_SERIALIZER = 'json.dumps'

# Specify the host and port to use when connecting to Redis (optional).
#REDIS_HOST = 'localhost'
#REDIS_PORT = 6379

# Specify the full Redis URL for connecting (optional).
# If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.
#REDIS_URL = 'redis://user:pass@hostname:9001'

# Custom redis client parameters (i.e.: socket timeout, etc.)
#REDIS_PARAMS  = {}
# Use custom redis client class.
#REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient'

# If True, it uses redis' ``spop`` operation. This could be useful if you
# want to avoid duplicates in your start urls list. In this cases, urls must
# be added via ``sadd`` command or you will get a type error from redis.
#REDIS_START_URLS_AS_SET = False

# Default start urls key for RedisSpider and RedisCrawlSpider.
#REDIS_START_URLS_KEY = '%(name)s:start_urls'

# Use other encoding than utf-8 for redis.
#REDIS_ENCODING = 'latin1'

USER_AGENT = 'scrapy-redis (+https://github.com/rolando/scrapy-redis)'

LOG_LEVEL = 'DEBUG'

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
DOWNLOAD_DELAY = 1
```

- 编写 tutorial/tutorial/spiders/myspider_redis.py 文件，如下:

```
from scrapy_redis.spiders import RedisSpider


class MySpider(RedisSpider):
    """Spider that reads urls from redis queue (myspider:start_urls)."""
    name = 'myspider_redis'
    redis_key = 'myspider:start_urls'

    def __init__(self, *args, **kwargs):
        # Dynamically define the allowed domains list.
        domain = kwargs.pop('domain', '')
        self.allowed_domains = filter(None, domain.split(','))
        super(MySpider, self).__init__(*args, **kwargs)

    def parse(self, response):
        for quote in response.xpath('//div[@class="quote"]'):
            yield{
                'text': quote.xpath('./span[@class="text"]/text()').extract_first(),
                'author': quote.xpath('.//small[@class="author"]/text()').extract_first(),
                'tags': quote.xpath('./div[@class="tags"]/a[@class="tag"]/text()').extract(),
            }
```

注意，上面代码中的name = 'myspider_redis' 与 redis_key = 'myspider:start_urls' 很重要。

3. 运行爬虫爬取信息

首先启动爬虫： myspider_redis，运行下列命令启动。

```
scrapy crawl myspider_redis


```
可以看到如下结果:
```
2019-07-31 17:10:52 [myspider_redis] INFO: Reading start URLs from redis key 'myspider:start_urls' (batch size: 16, encoding: utf-8

...

2019-07-31 17:10:53 [scrapy.core.engine] INFO: Spider opened
2019-07-31 17:10:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-07-31 17:10:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023

```
上面的输出表示，当前爬虫正在等待redis 服务器中提供start urls.

- 下面，在redis中设置键 myspider:start_urls，并赋值为 http://quotes.toscrape.com/

```
lpush myspider:start_urls http://quotes.toscrape.com/

```
这个key在输入后，很快将被上面的爬虫myspider监听到，然后就取出作为start url（消费掉），而将数据以myspider:items 列表作为结果存储在redis中


4. 查看结果


通过下面的命令查看：

```
127.0.0.1:6379> keys *
1) "myspider_redis:items"

127.0.0.1:6379> type myspider_redis:items
list

127.0.0.1:6379> llen myspider_redis:items
(integer) 20

127.0.0.1:6379> lrange myspider_redis:items 0 -1
 1) "{\"text\": \"\\u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.\\u201d\", \"author\": \"J.K. Rowling\", \"tags\": [\"abilities\", \"choices\"], \"crawled\": \"2019-07-31 08:08:01\", \"spider\": \"myspider_redis\"}"
 2) "{\"text\": \"\\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\\u201d\", \"author\": \"Albert Einstein\", \"tags\": [\"change\", \"deep-thoughts\", \"thinking\", \"world\"], \"crawled\": \"2019-07-31 08:08:01\", \"spider\": \"myspider_redis\"}"
 
 ...
 
```


#### 模块练习与答案

见习题集

#### 内容小结

本节介绍了编写分布式爬虫的一个简单实例，主要步骤有4步：

1. 启动redis数据库
2. 建立符合scrapy-redis要求的tutorial项目
3. 运行爬虫爬取信息
4. 查看结果

## 本节总结

本讲介绍了以下内容：

- 分布式系统概述
- scrapy-redis架构介绍
- 建立分布式爬虫的准备工作
- 分布式爬虫构建实例构建
- 运行爬虫与redis
- 查看结果

其中scrapy-redis的架构知识与分布式爬虫构建实操是重点。

## 课后练习

见习题集。
