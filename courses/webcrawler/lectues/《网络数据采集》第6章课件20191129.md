
# 网络数据采集

# 第七章 构建健壮的爬虫系统

- 讲师姓名：
- 授课时间：
- 共32课时，第23-28课时

## 1 上节回顾

上节课介绍了Selenium爬取网页的基本方法，同时介绍了几个案例。

## 2 本节课程主要内容

内容列表：
- 本节目标
- Scrapy爬虫框架基础
- scrapy具体操作与应用
- 本节总结
- 课后练习

### 2.1 本节目标

- 掌握Scrapy框架的构成要素
- 掌握Scrapy框架的工作原理
- 能够应用Scrapy设计网络数据采集系统

### 2.2 Scrapy爬虫框架基础

这一讲，我们介绍网络爬虫框架Scrapy的基本组成、工作原理，并介绍使用Scrapy构建较为健壮的网络爬虫应用程序的方法。

<img src="images/07/scrapylogo.png" width="480" alt="scrapylogo" />

#### 2.2.1 主要功能

[Scrapy](https://scrapy.org/)是一个爬虫框架，主要用于爬取网页、提取数据。可用于：
- 数据挖掘
- 网络监控
- 自动测试等 

<img src="images/07/scrapyfeatures.png" width="480" alt="scrapyfeatures" />

#### 2.2.2 Scrapy at a glance

为了从专业角度让大家了解Scrapy的基本应用方式，我们先看一个Scrapy的简单例子。

下面的例子以爬取示例网站 http://quotes.toscrape.com 中信息为目标，展现了使用Scrapy构建爬虫的基本方法。

**爬虫设计**
```
import scrapy


class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    start_urls = [
        'http://quotes.toscrape.com/tag/humor/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.xpath('span/small/text()').get(),
            }

        next_page = response.css('li.next a::attr("href")').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

**运行爬虫**
```scrapy runspider quotes_spider.py -o quotes.json```

**得到结果**
```
[{
    "author": "Jane Austen",
    "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"
},
{
    "author": "Groucho Marx",
    "text": "\u201cOutside of a dog, a book is man's best friend. Inside of a dog it's too dark to read.\u201d"
},
{
    "author": "Steve Martin",
    "text": "\u201cA day without sunshine is like, you know, night.\u201d"
},
...]
```

#### 2.2.3 Scrapy特性

从上面的例子中，我们可以看到使用Scrapy的优势：
- 网页请求经过调度和异步处理
  - 意味着scrapy不需要等待某个请求结束或处理完成，就同时可以发送另一个请求或做其他的工作。
- 可以构建多快速的多并发爬虫系统，并且有效控制爬虫的礼貌性。
- 可以在每个请求中，设置下载延迟，限制对每个IP的并发请求数量。
- 可以使用正则表达式、CSS、XPATH从htmlz中获取数据。
- 可以使用交互式shell控制台实现交互式开发过程。
- 支持多种输出格式（JSON、CSV、XML)
- 支持将输出结果导出到多个后端系统（文件、数据库）
- 强大的编码能力和自动检测能力，可以用于检查外来、非标的编码。
- 强大的扩展性支持，可以使用信号或良好的API，实现自定义功能。
- 丰富的内置中间件：
  - cookie
  - 会话处理
  - HTTP压缩、身份验证、缓存
  - Proxy
  - robots.txt
  - 爬行深度限制
- 支持远程控制台
- 大量可重用代码。


#### 2.2.4 Scrapy基本组成和工作原理。

Scrapy的主要模块有：

**Scrapy Engine(引擎)**

负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。

**Scheduler(调度器)**

它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。

**Downloader（下载器）**

负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理。

**Spider（爬虫）**

它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)。

**Item Pipeline(管道)**

它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。以流水线方式对Spider解析后得到的结果（Item），用户可以定义一组操作顺序，包括：清理、检验、查重、存储到数据库等。

**Downloader Middlewares（下载中间件）**

你可以当作是一个可以自定义扩展下载功能的组件。

**Spider Middlewares（Spider中间件）**

你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）,
下图是Scrapy的基本结构框图：

<img src="images/07/scrapy框架.png" width="480" alt="scrapy框架" />

借助这一幅图，我们再介绍一下scrapy的工作原理：

Scrapy有3条数据流路径：

**请求发起与调度**

- Spiders经Spider中间件向Engine发送Requests；
- Engine将爬取请求交给Scheduler，完成调度。

**请求执行与响应**

- Scheduler将调度后Requests交给Engine；
- Engine将请求转发给Downloader；
- Downloader执行下载并将响应Response对象提交给Engine；
- Engine将Response对象送给Spiders，用于提取Item和跟踪Links。

**结果输出与新请求的产生**

- Spiders处理Response对象，解析出结构化的爬得数据Item和新Links，并送给Engine；
- Engine将Items送给Item pipelines完成输出处理；
- Engine将新Requests送给Scheduler，产生新请求。


通过上面流程的分析，可知入口是Spiders、出口是Item pipelines、核心是Engine、策略由Scheduler制定。

在Scrapy中需要用户编写的模块类及方法有：

- Spiders
- Item pipelines

而Scrapy已经实现的部分（不需用户编写）是：

- Engine
- Scheduler
- Downloader

为了增加用户的灵活性，Scrapy在Engine与Spider之间、Engine与Downloader之间增加了中间件：Downloader Middleware，用户可以定义这些中间件来修改、丢弃和新增请求或响应。Spider Middleware，对请求和爬取项的再处理，可以修改、丢弃、增加请求和爬取项。

#### 2.2.5 模块练习与答案

见习题集

#### 2.2.6 内容小结

以上就是对scrapy基本组成和工作原理的介绍，希望大家能够细心体会，掌握这些内容将有利于我们后续基于scrapy设计网络爬虫程序。

### 2.3 scrapy具体操作与应用

下面通过具体操作介绍Scrapy基础应用方法。主要内容有：

- 准备工作
- 新建scrapy 项目
- 建立scrapy 爬虫
- 运行scrapy 爬虫
- 使用scrapy shell进行分析

下面我们将使用Scrapy爬取quotes.toscrape.com，中的这是一个收集了一些名人名言的网站。

#### 2.3.1 准备工作

为了在Anaconda3中借助scrapy框架编写爬虫程序，我们需要建立一个conda虚拟环境并安装scrapy框架：

1. 打开Anaconda prompt 命令行
2. 进入自己安装anaconda3的目录下的envs目录，查看当前已有的虚拟环境目录名
3. 运行下列命令，建立名为scrapyws的conda虚拟环境,并在其中安装scrapy

```
conda create -n scrapyws scrapy
```

4. 运行下列命令，激活该虚拟环境：
```
(base) D:\pythonspace\anaconda3\envs>conda activate scrapyws
## 运行后，会发现下面的路径前括号内的内容变化为scrapyws，正是我们建立的。
(scrapyws) D:\pythonspace\anaconda3\envs>
```
5. 运行下列命令进入scrapyws目录

```
(scrapyws) D:\pythonspace\anaconda3\envs>cd scrapyws

(scrapyws) D:\pythonspace\anaconda3\envs\scrapyws>dir`

## 你会发现这个目录下含有很多已安装的文件

```

> 注：如果不想使用虚拟环境，可以在Anaconda3命令行中直接运行```conda install -c conda-forge scrapy``` 或 ```pip install scrapy```。

Scrapy是一个纯python项目，它依赖的其他python packages：
- lxml, an efficient XML and HTML parser
- parsel, an HTML/XML data extraction library written on top of lxml,
- w3lib, a multi-purpose helper for dealing with URLs and web page encodings
- twisted, an asynchronous networking framework
- cryptography and pyOpenSSL, to deal with various network-level security needs

#### 2.3.2 新建 Scrapy 项目

在命令行执行下面语句就可以新建scrapy项目，在这里我们假定项目名为tutorial

```scrapy startproject tutorial```

如果你使用了上面推荐的conda虚拟环境安装scrapy，那么上面语句执行成功后，会在Anaconda的envs/myscrapy/中出现一个tutorial目录。

```
tutorial/
    scrapy.cfg            # 部署配置文件
    tutorial/             # 项目的python模块
        __init__.py
        items.py          # 项目输出结果的定义文件
        middlewares.py      # 项目中间件文件
        pipelines.py       # 项目管道文件
        settings.py       # 项目配置文件
        spiders/          # 爬虫文件目录
            __init__.py

```

各文件的基本功能是：

- scrapy.cfg ：项目的配置文件
- mySpider/ ：项目的Python模块，将会从这里引用代码
- mySpider/items.py ：项目的目标文件
- mySpider/pipelines.py ：项目的管道文件
- mySpider/settings.py ：项目的设置文件
- mySpider/spiders/ ：存储爬虫代码目录


我们将爬取quotes.toscrape.com中的名人名言，希望得到以下信息（JSON格式）：

```
{
    'author': 'Douglas Adams',
    'text': '“I may not have gone where I intended to go, but I think I ...”',
    'tags': ['life', 'navigation']
}
```

#### 2.3.3 编写爬虫文件

##### 生成具体爬虫程序

首先，在当前目录下输入命令，将tutorial/spider目录下创建一个名为quotesspider的爬虫，并制定爬取域的范围：

```tutorial/spider/> scrapy genspider quotesspider "quotes.toscrape.com"```

自定义的Spider类用于从网站上爬取信息，它需要继承scrapy.Spider，并生成初始请求。类的定义中还可以选择如何跟踪页内的链接，如何解析下载页面的内容。

##### 编写爬虫程序

编写下面的爬虫类:

```python
# -*- coding: utf-8 -*-
import scrapy

class QuotesspiderSpider(scrapy.Spider):
    name = "quotesspider"
    allowed_domains = ["quotes.toscrape.com"]
    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        for quote in response.xpath('//div[@class="quote"]'):
            yield {
                'text': quote.xpath('./span[@class="text"]/text()').extract_first(),
                'author': quote.xpath('.//small[@class="author"]/text()').extract_first(),
                'tags': quote.xpath('.//div[@class="tags"]/a[@class="tag"]/text()').extract()
            }

        next_page_url = response.xpath('//li[@class="next"]/a/@href').extract_first()
        if next_page_url is not None:
            yield scrapy.Request(response.urljoin(next_page_url))
```

**Python 中的 yield**

上面代码中使用了yield函数，它使它所在的函数成为了一个python generator。

所谓生成器也是一种产生可迭代对象的函数，它每次产生一个值后，函数就被冻结不再执行，当被唤醒时（需要获取下一个值时）才再次产生一个值。

例如：

```
def gen(n):
    for i in range(n):
        yield i**2
        
for i in gen(100000000):
    print(i)
```
生成器的优势：

- 内存更加优化
- 速度更快
- 应用更灵活

#### 运行Scrapy爬虫

为了使上面编写的爬虫程序工作，需要在命令行中进入项目顶级目录(否则会报错)并运行下面指令：

```scrapy crawl quotesspider```

这个命令将启动我们上面编写的名为 quotesspider 的爬虫，它将向quotes.toscrape.com网站发送请求，之后会得到类似下面的输出：

```
(scrapyws) D:\pythonspace\anaconda3\envs\scrapyws\tutorial\tutorial>scrapy crawl quotesspider
2019-07-30 10:36:58 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)
2019-07-30 10:36:58 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}
2019-07-30 10:36:58 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-07-30 10:36:58 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-07-30 10:36:58 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
 
 ......

2019-07-30 10:37:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/>
{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}
2019-07-30 10:37:00 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/>
{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}
......

```

#### 存储爬得数据

若要存储爬得数据，简单的方法是通过下面的命令将内容保存为json文件：

```scrapy crawl quotesspider -o quotes.json```

上面的命令会生成一个包含所有爬得数据条目（以JSON形式序列化）的qutoes.json文件。也可以使用别的存储格式，例如：

```scrapy crawl quotes -o quotes.jl```

由于JSON Lines格式是流状文件，所以你可以对其追加新的记录。JSON格式文件是不能追加内容的，如果不小心执行两次之前的命令，会得到一个破损的JSON文件。JSON Lines文件中的每条记录被分割为单独的一行，你可以使用它来在内存中处理大文件，而不需要应对大文件加载的一些麻烦。

如果要对爬得的数据条目执行更为复杂的操作，可以写一个Item Pipline类。一个服务于Item Pipelines的占位符文件在新建项目时就已被生成了，就存放在tutorial/pipelines.py文件中。这一点以后介绍。


#### 使用 scrapy shell 实现交互式爬取

上面的例子对scrapy的运作过程进行了简单介绍。可能不少同学会觉得这部分内容过于理想化。在没有对网站的分析和交互式反馈的情况下，我们不可能对scrapy有更好的应用。

下面我们尝试使用 scrapy shell 对http://quotes.toscrape.com 进行交互式的分析，然后根据分析过程和结果验证刚才的爬虫程序。

在scrapy工作环境（虚拟环境、scrapyws目录都要准备好）下运行Scrapy shell的命令是：

```scrapy shell "http://quotes.toscrape.com"```

注意：上面命令中要在URL外加引号，否则scrapy命令会认为是某个命令行参数。

运行命令后得到的命令行输出大致如下：

```
(scrapyws) D:\pythonspace\anaconda3\envs\scrapyws\tutorial>scrapy shell "http://quotes.toscrape.com"
2019-07-30 11:05:28 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: tutorial)
2019-07-30 11:05:28 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'tutorial', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'tutorial.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tutorial.spiders']}
2019-07-30 11:05:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
2019-07-30 11:05:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-07-30 11:05:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-07-30 11:05:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-07-30 11:05:28 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2019-07-30 11:05:28 [scrapy.core.engine] INFO: Spider opened
2019-07-30 11:05:30 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2019-07-30 11:05:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x0000029AB98B2E80>
[s]   item       {}
[s]   request    <GET http://quotes.toscrape.com>
[s]   response   <200 http://quotes.toscrape.com>
[s]   settings   <scrapy.settings.Settings object at 0x0000029ABA9B0B38>
[s]   spider     <QuotesspiderSpider 'quotesspider' at 0x29abac35f28>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>>
```
可以看到，上面的响应输出中含有：

> 2019-07-30 11:05:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com> (referer: None)

其中的200表示响应成功，我们可以使用下列命令快速查看scrapy下载的临时文件：

```>>>view(response)```

上述命令会打开系统默认浏览器，并加载scrapy下载到本地的临时页面。

之后，尝试运行:

```>>> response.xpath('//title')```

结果如下：
```
[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]
>>>
```

上面的命令会得到一个名为SelectorList的列表型对象。这个列表中的Selector对象会包装XML/HTML元素，以便于进行更多的查询来精确选择和提取数据。例如要提取上面例子中的title标签的文本，可以如下操作：

```>>> response.xpath('//title').extract_first()```

结果如下：
```
'<title>Quotes to Scrape</title>'
>>>

```

如果上面不写“::text”或“text()”,会得到整个title标签内容。上例中调用.extract()的结果是一个列表，因为我们处理的是一个SelectorList实例。如果你只想获得列表中的第一个内容，可以使用下列方法：

```>>> response.xpath('//title/text()').extract_first()```

结果如下：
```
'Quotes to Scrape'
>>>
```

另一种获得类似输出的方法是：

```response.css('title::text')[0].extract()```

或
```response.xpath('//title/text()')[0].extract()```

但是，使用.extract_first()方法可以在SelectorList为空时返回None，避免IndexError错误。

除了extract方法和extract_first()方法，还有re()方法支持使用正则表达式来选择文本并提取。

例如：

```response.xpath('//title/text()').re(r'Quotes')```

结果：['Quotes']

```response.xpath('//title/text()').re(r'Q\w+')```

结果：['Quotes']

```response.xpath('//title/text()').re(r'(\w+) to (\w+)')```

结果：['Quotes', 'Scrape']

为了能够找到正确的CSS选择器，你可以使用view(response)命令查看响应内容；或者使用Chrome或Firefox浏览器的开发者工具分析原页面的CSS样式或XPath表达式。XPath表达式非常强大，而且是Scrapy选择器Selectors的基础。事实上，CSS selectors会被转换为XPath。

##### 提取名人名言和作者

现在，我们了解了一些选择器selector和提取方法，下面尝试着完成名人名言爬虫的完整实现。http://quotes.toscrape.com 中的每个网页均有HTML元素组成，例如：

```
<div class="quote">
    <span class="text">“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”</span>
    <span>
        by <small class="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    <div class="tags">
        Tags:
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>
</div>
```
在设计爬虫解析函数前，我们首先要打开scrapy shell来找出如何提取我们想要的数据。可以运行如下命令：

```
scrapy shell "http://quotes.toscrape.com"
```

由于我们希望获取的信息包含在上面的div块中，所以我们尝试下列操作。运行下面命令将会得到一个名言HTML元素的selectors列表：

```
qs = response.css("div.quote")
或
qs = response.xpath('//div[@class="quote"]')
len(qs)

### 2.4 输出为10，表示爬取到了10条信息（div）
```


现在我们可以利用qs列表中第一个对象提取出子标签title、author、tags等内容：

```
>>> qs[0].xpath(".//span/text()")
[<Selector xpath='.//span/text()' data='“The world as we have created it is a pr'>, <Selector xpath='.//span/text()' data='by '>, <Selector xpath='.//span/text()' data='\n        '>, <Selector xpath='.//span/text()' data='\n        '>]
>>>

>>> qs[0].xpath('.//small[@class="author"]/text()').extract_first()
'Albert Einstein'
>>>

>>> qs[0].xpath('.//div[@class="tags"]/a[@class="tag"]/text()').extract()
['change', 'deep-thoughts', 'thinking', 'world']
>>>
```
说明：

- 上面的xpath路径前有“.”表示相对当前selector内容进行查找；
- qs[0]表示qs列表中的第一个元素
- extract_first()方法用于获取第一个selector对象的可打印内容；
- extract()方法获得的是所有可打印内容列表。



##### 跟踪链接

如果需要爬取整个网站的内容，就需要对页面中的链接进行获取和跟踪。如何使用Scrapy提取网页上的链接呢？

检查爬得的页面内容，我们可以看到有下一页的链接，例如：

```
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
    </li>
</ul>
```
为了保证程序的正确性，首先要在scrapy shell下进行测试：

```
>>> link = response.xpath('//li[@class="next"]/a/@href').extract_first()
>>> link
'/page/2/'
>>>
```

之后，就可以像上面的quotesspider中一样，写出递归爬取新URLs的方法.

##### 更多示例：进一步地爬取作者信息

下面的例子演示了多个回调函数和跟踪链接的情况：


```python
import scrapy


class AuthorSpider(scrapy.Spider):
    name = 'author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)'):
            yield response.follow(href, self.parse_author)

        # follow pagination links
        for href in response.css('li.next a::attr(href)'):
            yield response.follow(href, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).extract_first().strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
```

上面例子中的spider将调用parse_author()方法，从网站的main page开始爬取所有的指向作者页面的链接。在调用回调函数时，我们使用了:

> response.follow

这个方法可以使我们的代码更为精炼。


```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```



#### 内容小结

本节主要介绍了以下内容：

- conda虚拟环境建立
- scrapy安装
- scrapy 项目建立
- scrapy 爬虫生成
- scrapy 爬虫运行
- scrapy shell的使用


#### 模块练习与答案

见习题集

## 本节总结

本讲主要介绍了爬虫框架scrapy的基本原理和使用方法。

重点是：

- 深刻理解scrapy的组件结构，能够画出架构示意图
- 能够应用scrapy进行爬虫开发。


## 课后练习

见习题集。
